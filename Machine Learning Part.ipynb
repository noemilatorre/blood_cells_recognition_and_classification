{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749596d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file salvato in: logs/20250708_153353/model_evaluation.log\n",
      "\n",
      "========================================\n",
      "Random Search + Training Stage 1: Healthy vs Infected (SVM)\n",
      "========================================\n",
      "Running RandomizedSearchCV for Stage 1 - SVM ...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achille/Desktop/ML/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END classifier__C=100, classifier__gamma=auto, classifier__kernel=poly, selector__k=700; total time=15.0min\n",
      "[CV] END classifier__C=100, classifier__gamma=auto, classifier__kernel=poly, selector__k=700; total time=20.4min\n",
      "[CV] END classifier__C=100, classifier__gamma=auto, classifier__kernel=poly, selector__k=700; total time=25.9min\n",
      "Best parameters for Stage 1 - SVM: {'selector__k': 700, 'classifier__kernel': 'poly', 'classifier__gamma': 'auto', 'classifier__C': 100}\n",
      "Best F1 Macro Score for Stage 1 - SVM: 0.7517\n",
      "\n",
      "--- Evaluation: Stage 1 - SVM ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98     14195\n",
      "           1       0.40      0.66      0.50       507\n",
      "\n",
      "    accuracy                           0.95     14702\n",
      "   macro avg       0.70      0.81      0.74     14702\n",
      "weighted avg       0.97      0.95      0.96     14702\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13696   499]\n",
      " [  170   337]]\n",
      "Balanced Accuracy: 0.8147705285565312\n",
      "F1 Macro: 0.7390102930476189\n",
      "--------------------------------------------------------------------------------\n",
      "Number of classes in Stage 2: 5\n",
      "\n",
      "========================================\n",
      "Random Search + Training Stage 2: Infected Subtype Classification (SVM ➡ SVM)\n",
      "========================================\n",
      "Running RandomizedSearchCV for Stage 2 - SVM ➡ SVM ...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achille/Desktop/ML/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END classifier__C=100, classifier__gamma=auto, classifier__kernel=poly, selector__k=700; total time=  16.4s\n",
      "[CV] END classifier__C=100, classifier__gamma=auto, classifier__kernel=poly, selector__k=700; total time=  20.1s\n",
      "[CV] END classifier__C=100, classifier__gamma=auto, classifier__kernel=poly, selector__k=700; total time=  19.2s\n",
      "Best parameters for Stage 2 - SVM ➡ SVM: {'selector__k': 700, 'classifier__kernel': 'poly', 'classifier__gamma': 'auto', 'classifier__C': 100}\n",
      "Best F1 Macro Score for Stage 2 - SVM ➡ SVM: 0.2946\n",
      "\n",
      "--- Evaluation: Stage 2 - SVM ➡ SVM ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.53      0.62       193\n",
      "           1       0.19      0.22      0.21        18\n",
      "           2       0.31      0.53      0.39        66\n",
      "           3       0.24      0.36      0.29        25\n",
      "           4       0.13      0.10      0.11        31\n",
      "\n",
      "    accuracy                           0.46       333\n",
      "   macro avg       0.32      0.35      0.32       333\n",
      "weighted avg       0.53      0.46      0.48       333\n",
      "\n",
      "Confusion Matrix:\n",
      "[[102  14  52  16   9]\n",
      " [  7   4   5   0   2]\n",
      " [ 14   0  35  10   7]\n",
      " [  5   1   8   9   2]\n",
      " [ 10   2  14   2   3]]\n",
      "Balanced Accuracy: 0.3475593710800129\n",
      "F1 Macro: 0.32235299703382736\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 323\u001b[0m\n\u001b[1;32m    321\u001b[0m         handler\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 323\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 316\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# Run GridSearchCV\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     best_pipeline_stage2 \u001b[38;5;241m=\u001b[39m run_random_search(pipeline_stage2, param_dist_stage2, X_train_stage2, y_train_stage2_encoded, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStage 2 - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ➡ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 316\u001b[0m     \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_pipeline_stage2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_stage2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_stage2_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStage 2 - \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname1\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m ➡ \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname2\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Pipeline with Random Search completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;241m.\u001b[39mhandlers[:]:\n",
      "Cell \u001b[0;32mIn[1], line 102\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, X_test, y_test, model_name)\u001b[0m\n\u001b[1;32m     98\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    101\u001b[0m y_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Probability of positive class\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m precision, recall, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Plot\u001b[39;00m\n\u001b[1;32m    105\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(recall, precision)\n",
      "File \u001b[0;32m~/Desktop/ML/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/ML/.venv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1005\u001b[0m, in \u001b[0;36mprecision_recall_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate, probas_pred)\u001b[0m\n\u001b[1;32m    996\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    997\u001b[0m         (\n\u001b[1;32m    998\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobas_pred was deprecated in version 1.5 and will be removed in 1.7.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1002\u001b[0m     )\n\u001b[1;32m   1003\u001b[0m     y_score \u001b[38;5;241m=\u001b[39m probas_pred\n\u001b[0;32m-> 1005\u001b[0m fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# Drop thresholds corresponding to points where true positives (tps)\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# do not change from the previous or subsequent point. This will keep\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;66;03m# only the first and last point for each tps value. All points\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# with the same tps value have the same recall and thus x coordinate.\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# They appear as a vertical line on the plot.\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     optimal_idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[1;32m   1016\u001b[0m         np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m   1017\u001b[0m             [[\u001b[38;5;28;01mTrue\u001b[39;00m], np\u001b[38;5;241m.\u001b[39mlogical_or(np\u001b[38;5;241m.\u001b[39mdiff(tps[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), np\u001b[38;5;241m.\u001b[39mdiff(tps[\u001b[38;5;241m1\u001b[39m:])), [\u001b[38;5;28;01mTrue\u001b[39;00m]]\n\u001b[1;32m   1018\u001b[0m         )\n\u001b[1;32m   1019\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/ML/.venv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:818\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    816\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m    820\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m    821\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, balanced_accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "svm = False\n",
    "\n",
    "# === Create timestamped logging folder ===\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_dir = f\"logs/{timestamp}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# === Logging setup ===\n",
    "log_file = os.path.join(log_dir, \"model_evaluation.log\")\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logging.info(\"Logger inizializzato con successo.\")\n",
    "\n",
    "def save_plot(fig, name):\n",
    "    fig_path = os.path.join(log_dir, name)\n",
    "    fig.savefig(fig_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "print(f\"Log file salvato in: {log_file}\")\n",
    "\n",
    "def recode_labels_for_first_classifier(labels):\n",
    "    return labels.apply(lambda x: \"healthy\" if x in [0, 1] else \"infected\")\n",
    "\n",
    "def filter_data_for_second_classifier(X, y):\n",
    "    mask = y.isin([2, 3, 4, 5, 6])\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "def evaluate_model(model, X_test, y_test,num_class, model_name=\"Model\"):\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"\\n--- Evaluation: {model_name} ---\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(matrix)\n",
    "    print(f\"Balanced Accuracy: {balanced_acc}\")\n",
    "    print(f\"F1 Macro: {f1_macro}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    logging.info(f\"\\n--- Evaluation: {model_name} ---\")\n",
    "    logging.info(f\"Classification Report:\\n{report}\")\n",
    "    logging.info(f\"Confusion Matrix:\\n{matrix}\")\n",
    "    logging.info(f\"Balanced Accuracy: {balanced_acc}\")\n",
    "    logging.info(f\"F1 Macro: {f1_macro}\")\n",
    "    logging.info(\"-\" * 80)\n",
    "\n",
    "    # figures\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    save_plot(plt.gcf(), f\"confusion_matrix_{model_name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "    # Plot\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    save_plot(plt.gcf(), f\"precision_recall_curve_{model_name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # scatter plot lda\n",
    "    plt.figure (figsize=(10, 8))\n",
    "    lda = LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)\n",
    "    X_lda = lda.fit_transform(X_test, y_test)\n",
    "    plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y_test, cmap='viridis', edgecolor='k', s=50)\n",
    "    plt.title(f'LDA Scatter Plot - {model_name}')\n",
    "    plt.xlabel('LDA Component 1')\n",
    "    plt.ylabel('LDA Component 2')\n",
    "    save_plot(plt.gcf(), f\"lda_scatter_plot_{model_name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Pipeline 1: XGBoost\n",
    "def create_xgb_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif, k=100)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', XGBClassifier(eval_metric='mlogloss', random_state=42))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 6, 10],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    return pipeline, param_dist\n",
    "\n",
    "# Pipeline 2: Random Forest\n",
    "def create_rf_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    return pipeline, param_dist\n",
    "\n",
    "# Pipeline 3: SVM\n",
    "def create_svm_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif, k=100)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', SVC(random_state=42, class_weight='balanced', kernel='linear', probability=True))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__C': [100],\n",
    "        'classifier__kernel': ['poly'],\n",
    "        'classifier__gamma': ['auto'] \n",
    "    }\n",
    "    return pipeline, param_dist\n",
    "\n",
    "# Pipeline 4: Logistic Regression\n",
    "def create_lr_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif, k=100)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', LogisticRegression(random_state=42, class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__solver': ['liblinear', 'saga']\n",
    "    }\n",
    "\n",
    "    return pipeline, param_dist\n",
    "\n",
    "# Pipeline 5: KNN\n",
    "def create_knn_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif, k=100)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__p': [1, 2]  # 1: manhattan, 2: euclidean\n",
    "    }\n",
    "\n",
    "    return pipeline, param_dist\n",
    "\n",
    "def run_random_search(pipeline, param_dist, X_train, y_train, model_name=\"Model\", n_iter=10):\n",
    "    print(f\"Running RandomizedSearchCV for {model_name} ...\")\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        scoring='f1_macro',\n",
    "        n_iter=n_iter,           # Number of random combinations to try\n",
    "        n_jobs=2,                # Parallel jobs (adjust based on RAM)\n",
    "        cv=3,                    # Cross-validation folds\n",
    "        verbose=2,\n",
    "        random_state=42,         # For reproducibility\n",
    "        refit=True\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for {model_name}: {random_search.best_params_}\")\n",
    "    print(f\"Best F1 Macro Score for {model_name}: {random_search.best_score_:.4f}\")\n",
    "    logging.info(f\"Best parameters for {model_name}: {random_search.best_params_}\")\n",
    "    logging.info(f\"Best F1 Macro Score for {model_name}: {random_search.best_score_:.4f}\")\n",
    "    return random_search.best_estimator_\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(\"../roi_features_train.csv\")\n",
    "    X = df.drop(columns=[\"image_id\", \"score\", \"x1\", \"y1\", \"x2\", \"y2\", \"label\"])\n",
    "    y_original = df[\"label\"]\n",
    "\n",
    "    y_stage1 = recode_labels_for_first_classifier(y_original)\n",
    "    le_stage1 = LabelEncoder()\n",
    "    y_stage1_encoded = le_stage1.fit_transform(y_stage1)\n",
    "\n",
    "    X_train_stage1, X_test_stage1, y_train_stage1, y_test_stage1 = train_test_split(\n",
    "        X, y_stage1_encoded, test_size=0.2, stratify=y_stage1_encoded, random_state=42\n",
    "    )\n",
    "\n",
    "    pipelines = [\n",
    "        (\"XGBoost\", create_xgb_pipeline),\n",
    "        (\"Random Forest\", create_rf_pipeline),\n",
    "        (\"SVM\", create_svm_pipeline),\n",
    "        (\"Logistic Regression\", create_lr_pipeline),\n",
    "        (\"KNN\", create_knn_pipeline)\n",
    "    ]\n",
    "\n",
    "    # to fix SVM on the first stage\n",
    "    name1 = 'SVM' \n",
    "    pipeline_stage1, param_dist_stage1 = create_svm_pipeline(num_class=2)\n",
    "    logging.info(f\"\\n{'='*40}\\nRandom Search + Training Stage 1: Healthy vs Infected ({name1})\\n{'='*40}\")\n",
    "    print(f\"\\n{'='*40}\\nRandom Search + Training Stage 1: Healthy vs Infected ({name1})\\n{'='*40}\")\n",
    "\n",
    "    # Run random_search\n",
    "    best_pipeline_stage1 = run_random_search(pipeline_stage1, param_dist_stage1, X_train_stage1, y_train_stage1, model_name=f\"Stage 1 - {name1}\")\n",
    "    \n",
    "    evaluate_model(best_pipeline_stage1, X_test_stage1, y_test_stage1,2, model_name=f\"Stage 1 - {name1}\")\n",
    "\n",
    "    y_pred_stage1 = best_pipeline_stage1.predict(X_test_stage1)\n",
    "    infected_indices = np.where(y_pred_stage1 == 1)[0]\n",
    "\n",
    "\n",
    "    X_test_stage2 = X_test_stage1.iloc[infected_indices]\n",
    "    allowed_labels = [2, 3, 4, 5, 6]\n",
    "    mask_test_stage2 = y_original.iloc[X_test_stage2.index].isin(allowed_labels)\n",
    "    X_test_stage2 = X_test_stage2[mask_test_stage2]\n",
    "    y_test_stage2 = y_original.iloc[X_test_stage2.index]\n",
    "\n",
    "    X_train_stage2, y_train_stage2 = filter_data_for_second_classifier(\n",
    "        X_train_stage1, y_original.iloc[X_train_stage1.index]\n",
    "    )\n",
    "\n",
    "    le_stage2 = LabelEncoder()\n",
    "    y_train_stage2_encoded = le_stage2.fit_transform(y_train_stage2)\n",
    "    \n",
    "    y_test_stage2_encoded = le_stage2.transform(y_test_stage2)\n",
    "    \n",
    "\n",
    "    num_classes_stage2 = len(le_stage2.classes_)\n",
    "    logging.info(f\"Number of classes in Stage 2: {num_classes_stage2}\")\n",
    "    print(f\"Number of classes in Stage 2: {num_classes_stage2}\")\n",
    "\n",
    "    for name2, pipeline_func2 in pipelines:\n",
    "        logging.info(f\"\\n{'='*40}\\nRandom Search + Training Stage 2: Infected Subtype Classification ({name1} ➡ {name2})\\n{'='*40}\")\n",
    "        print(f\"\\n{'='*40}\\nRandom Search + Training Stage 2: Infected Subtype Classification ({name1} ➡ {name2})\\n{'='*40}\")\n",
    "        pipeline_stage2, param_dist_stage2 = pipeline_func2(num_class=num_classes_stage2)\n",
    "\n",
    "        # Run random_search\n",
    "        best_pipeline_stage2 = run_random_search(pipeline_stage2, param_dist_stage2, X_train_stage2, y_train_stage2_encoded, model_name=f\"Stage 2 - {name1} ➡ {name2}\")\n",
    "\n",
    "        evaluate_model(best_pipeline_stage2, X_test_stage2, y_test_stage2_encoded,num_classes_stage2, model_name=f\"Stage 2 - {name1} ➡ {name2}\")\n",
    "\n",
    "    print(\"✅ Pipeline with Random Search completed successfully.\")\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        handler.flush()\n",
    "        handler.close()\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
